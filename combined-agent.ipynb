{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "%matplotlib inline\n",
    "\n",
    "from variables import maze_list, Actions\n",
    "\n",
    "import environment\n",
    "from environment import maze_game\n",
    "\n",
    "from tiered_algorithm import words_to_actions, actions_to_words, state_to_actions\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "environment.set_device(device)\n",
    "\n",
    "print(f'GPU available: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = [\n",
    "    'move',\n",
    "    'up',\n",
    "    'left',\n",
    "    'right',\n",
    "    'down',\n",
    "    'one',\n",
    "    'two',\n",
    "    'three',\n",
    "    'blocks',\n",
    "    '',\n",
    "    'and',\n",
    "    'then',\n",
    "    '1',\n",
    "    '2',\n",
    "    '3',\n",
    "    '4',\n",
    "    '5',\n",
    "    'four',\n",
    "    'five',\n",
    "    'block',\n",
    "    'north',\n",
    "    'south',\n",
    "    'east',\n",
    "    'west',\n",
    "    'go',\n",
    "    'walk',\n",
    "    'proceed',\n",
    "    'step',\n",
    "    'steps',\n",
    "    'space',\n",
    "    'spaces',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1]],\n",
       "\n",
       "        [[4]],\n",
       "\n",
       "        [[4]],\n",
       "\n",
       "        [[4]]], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "game = maze_game([maze_list[0]])\n",
    "game.reset()\n",
    "game.set_target(np.array([4,4]))\n",
    "game.show()\n",
    "game.state_to_channels(game.state).shape\n",
    "# game.valid_actions()\n",
    "game.best_seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, maze, start=None, target=None, max_episodes=100, show_game=False, epsilon=0):\n",
    "    game = maze_game([maze])\n",
    "    game.reset()\n",
    "    \n",
    "    if start is not None:\n",
    "        game.set_position(start)\n",
    "    if target is not None:\n",
    "        game.set_target(target)\n",
    "        \n",
    "    while game.is_complete():\n",
    "        game.reset()\n",
    "        if start is not None:\n",
    "            game.set_position(start)\n",
    "        if target is not None:\n",
    "            game.set_target(target)\n",
    "        \n",
    "    episode = 0\n",
    "    envstate = game.get_state()\n",
    "    while episode < max_episodes:\n",
    "        episode += 1\n",
    "        valid_actions = game.valid_actions()\n",
    "        \n",
    "        if np.random.rand() < epsilon or model==None:\n",
    "            actions = [random.choice(valid_actions)]\n",
    "        else:\n",
    "            action = sender_net(game.get_state()[None]).max(2)[1].view(1, sequence_length, 1, 1)[0]\n",
    "#             sentence = [bow[i.item()] for i in action]\n",
    "\n",
    "            enc = torch.eye(len(bow))[[i.item() for i in action]].view(len(action),len(bow)).to(device)\n",
    "            actions = policy_net(enc[None]).max(2)[1].view(1, max_actions, 1, 1)[0]\n",
    "            actions = [Actions(i.item()) for i in actions]\n",
    "        \n",
    "        envstate, reward, done, win = game.step(actions)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if show_game: game.show()\n",
    "    \n",
    "    return win, episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 75)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "play_game(None, maze_list[0], show_game=True, target=np.array([4,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b00636a4f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "game.step(state_to_actions(game.state, limit_n=4))\n",
    "game.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 4\n",
    "max_actions = 4\n",
    "n_inputs = len(bow)\n",
    "\n",
    "class DQN_SENDER(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN_SENDER, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3,3), stride=1)\n",
    "        self.relu1 = nn.ReLU(inplace=False)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3,3), stride=1)\n",
    "        self.relu2 = nn.ReLU(inplace=False)\n",
    "        \n",
    "        convw = 4\n",
    "        convh = 4\n",
    "        linear_conv_size = convw * convh * 64\n",
    "        \n",
    "        self.gru = nn.GRU(linear_conv_size, linear_conv_size, 2)\n",
    "        \n",
    "        self.head = nn.Linear(linear_conv_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        # Flatten\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x, hn = self.gru(x.repeat(sequence_length,1,1))\n",
    "        \n",
    "        return self.head(x.transpose(0,1))\n",
    "\n",
    "class DQN_RECEIVER(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN_RECEIVER, self).__init__()\n",
    "        \n",
    "        convw = 4\n",
    "        convh = 4\n",
    "        linear_conv_size = convw * convh * 64\n",
    "        \n",
    "        self.gru = nn.GRU(n_inputs, linear_conv_size, 2)\n",
    "        self.gru2 = nn.GRU(linear_conv_size, linear_conv_size, 2)\n",
    "        \n",
    "        self.head = nn.Linear(linear_conv_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        # Flatten\n",
    "        x, hn = self.gru(x.transpose(0,1))\n",
    "        \n",
    "        x, hn = self.gru2(hn[1].repeat(max_actions,1,1))\n",
    "        \n",
    "        return self.head(x.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.9\n",
    "EPS_START = 0.4\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 5000\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "\n",
    "screen_height, screen_width = maze_list[0].shape\n",
    "\n",
    "n_actions = 5\n",
    "\n",
    "policy_net = DQN_RECEIVER(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN_RECEIVER(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr=5e-5)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state, valid_actions=[]):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state[None]).max(2)[1].view(1, max_actions, 1, 1)[0]\n",
    "    else:\n",
    "        if len(valid_actions) > 0:\n",
    "            return torch.tensor([[random.choice(valid_actions).value]], device=device, dtype=torch.long).unsqueeze(1).repeat(max_actions,1,1)\n",
    "        return torch.tensor([[random.randrange(n_actions, sequence_length)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "map_num = 0\n",
    "import pickle\n",
    "with open(f'models/sender.pkl', 'rb') as f:\n",
    "    sender_net = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "#     return\n",
    "    optimizer.zero_grad()\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.stack([s for s in batch.next_state\n",
    "                                                if s is not None], dim=0)\n",
    "    \n",
    "    state_batch = torch.stack(batch.state, dim=0)\n",
    "    action_batch = torch.stack(batch.action, dim=0)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_actions = policy_net(state_batch)\n",
    "    \n",
    "    state_actions = torch.stack([state_actions[:,i].gather(1, action_batch[:,i]) for i in range(sequence_length)], dim=1)\n",
    "    \n",
    "    state_action_values = state_actions.sum(dim=1)\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    \n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(2)[0].sum(dim=1).detach()\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    loss.backward()\n",
    "#     for param in policy_net.parameters():\n",
    "#         param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "optimize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_bow(seq):\n",
    "    return torch.eye(len(bow))[[bow.index(s) for s in seq]].view(len(seq),len(bow)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.autograd.detect_anomaly():\n",
    "# save_file='2020_10_08_receiver_3'\n",
    "# save_skip=2500\n",
    "num_episodes = 5000\n",
    "mb = progress_bar(range(num_episodes))\n",
    "p_skip = 100\n",
    "\n",
    "# env = maze_game([maze_list[map_num]])\n",
    "env = maze_game(maze_list)\n",
    "\n",
    "w_hist = []\n",
    "r_hist = []\n",
    "\n",
    "# for i_episode in mb:\n",
    "for i_episode in mb:\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    env.set_target(np.array([4,4]))\n",
    "    while env.is_complete():\n",
    "        env.reset()\n",
    "        env.set_target(np.array([4,4]))\n",
    "\n",
    "    a = sender_net(env.get_state()[None]).max(2)[1].view(1, sequence_length, 1, 1)[0]\n",
    "    sentence = [bow[i.item()] for i in a]\n",
    "    sentence += ['']*(sequence_length - len(sentence))\n",
    "    \n",
    "    last_state = encode_bow(sentence)\n",
    "    state = last_state\n",
    "    loss = []\n",
    "    for t in count():\n",
    "        \n",
    "        # Select and perform an action\n",
    "        valid_actions = env.valid_actions()\n",
    "        \n",
    "        action = select_action(state, valid_actions=valid_actions)\n",
    "        \n",
    "\n",
    "        actions = [Actions(i.item()) for i in action]\n",
    "    \n",
    "        a_reward = 0\n",
    "        \n",
    "        r = max_actions\n",
    "        for a,b in enumerate(actions):\n",
    "            if b != Actions.NONE:\n",
    "                r = a\n",
    "                \n",
    "        actions = actions[:r+1]\n",
    "        \n",
    "        action = action.view(max_actions,1)\n",
    "        \n",
    "        _, reward, done, win = env.step(actions)\n",
    "        reward += a_reward\n",
    "        \n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        r_hist += [reward.item()]\n",
    "        # Observe new state\n",
    "        if not done:\n",
    "            a = sender_net(env.get_state()[None]).max(2)[1].view(1, sequence_length, 1, 1)[0]\n",
    "            sentence = [bow[i.item()] for i in a]\n",
    "            sentence += ['']*(sequence_length - len(sentence))\n",
    "        \n",
    "            next_state = encode_bow(sentence)\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        loss += [optimize_model()]\n",
    "\n",
    "        if done:\n",
    "            w_hist.append(win)\n",
    "            episode_durations.append(t + 1)\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "#     print(f\"episode: {i_episode:3d}/{num_episodes-1} | steps: {t+1:3d} | wins: {sum(w_hist):3d} | Epsilon: {eps_threshold:.3f}\")\n",
    "#     if (i_episode+1)%save_skip==0:\n",
    "#         with open(f'models/{save_file}_policy.pkl', 'wb') as f:\n",
    "#             pickle.dump(policy_net, f)\n",
    "#         with open(f'models/{save_file}_target.pkl', 'wb') as f:\n",
    "#             pickle.dump(target_net, f)\n",
    "\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "        \n",
    "        \n",
    "    if (i_episode+1)%p_skip == 0:\n",
    "        print(f\"Episode: {i_episode:4d}/{num_episodes-1} | Rounds: {env.rounds:3d} | Steps: {env.steps:3d} | Wins: {sum(w_hist):4d} | Epsilon: {eps_threshold:.3f} | Avg Episode Reward: {f'{np.mean(r_hist[-(t+1):]):.4f}':>7s} | Avg Reward: {f'{np.mean(r_hist[-200:]):.4f}':>7s} | Total Winrate: {f'{sum(w_hist)/(i_episode+1)*100:>3.1f}':>5s}% | Recent Winrate: {f'{sum(w_hist[-p_skip:])/(p_skip)*100:>3.1f}':>5s}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sender_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-308f6badf072>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_position\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msender_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0menc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencode_bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sender_net' is not defined"
     ]
    }
   ],
   "source": [
    "map_num = 2\n",
    "env = maze_game([maze_list[map_num]])\n",
    "env.reset()\n",
    "env.set_target(np.array([4,4]))\n",
    "env.set_position(np.array([0,1]))\n",
    "\n",
    "a = sender_net(env.get_state()[None]).max(2)[1].view(1, sequence_length, 1, 1)[0]\n",
    "sentence = [bow[i.item()] for i in a]\n",
    "enc = encode_bow(sentence)\n",
    "\n",
    "action = policy_net(enc[None]).max(2)[1].view(1, max_actions, 1, 1)[0]\n",
    "\n",
    "actions = [Actions(i.item()) for i in action]\n",
    "print(f\"Sentence: '{' '.join(sentence)}'\")\n",
    "\n",
    "if len(actions) == 0:\n",
    "    actions = [Actions.NONE]\n",
    "env.step(actions)\n",
    "env.show()\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2000' class='' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [2000/2000 00:34<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Win Rate: 97.75%, Avg Steps: 3.6475\n",
      "Maze 0:  87.61%, 6.99 steps\n",
      "Maze 1: 100.00%, 3.52 steps\n",
      "Maze 2: 100.00%, 2.48 steps\n",
      "Maze 3: 100.00%, 4.10 steps\n",
      "Maze 4:  98.74%, 2.76 steps\n",
      "Maze 5: 100.00%, 2.01 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-48-51c51ad03891>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  print(f\"Total Win Rate: {np.mean(np.concatenate(np.array(w_hist)))*100:.2f}%, Avg Steps: {np.mean(np.concatenate(np.array(s_hist)))}\")\n"
     ]
    }
   ],
   "source": [
    "mb = progress_bar(range(2000))\n",
    "# m = [maze_list[map_num]]\n",
    "m = maze_list\n",
    "w_hist = [[] for i in range(len(m))]\n",
    "s_hist = [[] for i in range(len(m))]\n",
    "for i in mb:\n",
    "    maze_num = np.random.randint(0, len(m))\n",
    "    win,steps = play_game(policy_net, m[maze_num], epsilon=0.01, target=np.array([4,4]))\n",
    "    w_hist[maze_num] = np.append(w_hist[maze_num], win)\n",
    "    s_hist[maze_num] = np.append(s_hist[maze_num], steps)\n",
    "\n",
    "print(f\"Total Win Rate: {np.mean(np.concatenate(np.array(w_hist)))*100:.2f}%, Avg Steps: {np.mean(np.concatenate(np.array(s_hist)))}\")\n",
    "for i,j in enumerate(w_hist):\n",
    "    print(f\"Maze {i}: {f'{np.mean(j)*100:.2f}':>6s}%, {np.mean(s_hist[i]):.2f} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('models/combined_receiver_policy.pkl', 'wb') as f:\n",
    "    pickle.dump(policy_net, f)\n",
    "with open('models/combined_receiver_target.pkl', 'wb') as f:\n",
    "    pickle.dump(target_net, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('models/combined_receiver_policy.pkl', 'rb') as f:\n",
    "    policy_net = pickle.load(f)\n",
    "with open('models/combined_receiver_target.pkl', 'rb') as f:\n",
    "    target_net = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
