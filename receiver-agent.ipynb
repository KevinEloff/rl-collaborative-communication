{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "%matplotlib inline\n",
    "\n",
    "from variables import maze_list, Actions\n",
    "\n",
    "import environment\n",
    "from environment import maze_game\n",
    "\n",
    "from tiered_algorithm import words_to_actions, actions_to_words, state_to_actions\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "environment.set_device(device)\n",
    "\n",
    "print(f'GPU available: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = [\n",
    "    'move',\n",
    "    'up',\n",
    "    'left',\n",
    "    'right',\n",
    "    'down',\n",
    "    'one',\n",
    "    'two',\n",
    "    'three',\n",
    "    'blocks',\n",
    "    '',\n",
    "    'and',\n",
    "    'then',\n",
    "    '1',\n",
    "    '2',\n",
    "    '3',\n",
    "    '4',\n",
    "    '5',\n",
    "    'four',\n",
    "    'five',\n",
    "    'block',\n",
    "    'north',\n",
    "    'south',\n",
    "    'east',\n",
    "    'west',\n",
    "    'go',\n",
    "    'walk',\n",
    "    'proceed',\n",
    "    'step',\n",
    "    'steps',\n",
    "    'space',\n",
    "    'spaces',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2]],\n",
       "\n",
       "        [[2]],\n",
       "\n",
       "        [[4]],\n",
       "\n",
       "        [[4]]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "game = maze_game([maze_list[0]])\n",
    "game.reset()\n",
    "game.set_target(np.array([4,4]))\n",
    "game.show()\n",
    "game.state_to_channels(game.state).shape\n",
    "# game.valid_actions()\n",
    "game.best_seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, maze, start=None, target=None, max_episodes=100, show_game=False, epsilon=0):\n",
    "    game = maze_game([maze])\n",
    "    game.reset()\n",
    "    \n",
    "    if start is not None:\n",
    "        game.set_position(start)\n",
    "    if target is not None:\n",
    "        game.set_target(target)\n",
    "        \n",
    "    while game.is_complete():\n",
    "        game.reset()\n",
    "        if start is not None:\n",
    "            game.set_position(start)\n",
    "        if target is not None:\n",
    "            game.set_target(target)\n",
    "        \n",
    "    episode = 0\n",
    "    envstate = game.get_state()\n",
    "    while episode < max_episodes:\n",
    "        episode += 1\n",
    "        valid_actions = game.valid_actions()\n",
    "        \n",
    "        if np.random.rand() < epsilon or model==None:\n",
    "            actions = [random.choice(valid_actions)]\n",
    "        else:\n",
    "            sentence = actions_to_words(state_to_actions(game.state))\n",
    "            enc = encode_bow(sentence)\n",
    "            # print(enc)\n",
    "            relative_time = time.time()\n",
    "            action = policy_net(enc[None]).max(2)[1].view(1, max_actions, 1, 1)[0]\n",
    "            actions = [Actions(i.item()) for i in action]\n",
    "        \n",
    "        envstate, reward, done, win = game.step(actions)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if show_game: game.show()\n",
    "    \n",
    "    return win, episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "play_game(None, maze_list[0], show_game=True, target=np.array([4,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e3aee77130>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "game.step(state_to_actions(game.state, limit_n=4))\n",
    "game.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 4\n",
    "max_actions = 4\n",
    "n_inputs = len(bow)\n",
    "\n",
    "class DQN_RECEIVER(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN_RECEIVER, self).__init__()\n",
    "        \n",
    "        convw = 4\n",
    "        convh = 4\n",
    "        linear_conv_size = convw * convh * 64\n",
    "        \n",
    "        self.gru = nn.GRU(n_inputs, linear_conv_size, 2)\n",
    "        self.gru2 = nn.GRU(linear_conv_size, linear_conv_size, 2)\n",
    "        \n",
    "        self.head = nn.Linear(linear_conv_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x, hn = self.gru(x.transpose(0,1))\n",
    "        x, hn = self.gru2(hn[1].repeat(max_actions,1,1))\n",
    "        \n",
    "        return self.head(x.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.9\n",
    "EPS_START = 0.9\n",
    "EPS_START = 0.4\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 5000\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "\n",
    "screen_height, screen_width = maze_list[0].shape\n",
    "\n",
    "n_actions = 5\n",
    "\n",
    "policy_net = DQN_RECEIVER(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN_RECEIVER(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr=5e-5)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state, valid_actions=[]):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state[None]).max(2)[1].view(1, max_actions, 1, 1)[0]\n",
    "    else:\n",
    "        if len(valid_actions) > 0:\n",
    "            return torch.tensor([[random.choice(valid_actions).value]], device=device, dtype=torch.long).unsqueeze(1).repeat(max_actions,1,1)\n",
    "        return torch.tensor([[random.randrange(n_actions, sequence_length)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "#     return\n",
    "    optimizer.zero_grad()\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.stack([s for s in batch.next_state\n",
    "                                                if s is not None], dim=0)\n",
    "    \n",
    "    state_batch = torch.stack(batch.state, dim=0)\n",
    "    action_batch = torch.stack(batch.action, dim=0)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_actions = policy_net(state_batch)\n",
    "    \n",
    "    state_actions = torch.stack([state_actions[:,i].gather(1, action_batch[:,i]) for i in range(sequence_length)], dim=1)\n",
    "    \n",
    "    state_action_values = state_actions.sum(dim=1)\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    \n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(2)[0].sum(dim=1).detach()\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    loss.backward()\n",
    "#     for param in policy_net.parameters():\n",
    "#         param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_bow(seq):\n",
    "    return torch.eye(len(bow))[[bow.index(s) for s in seq]].view(len(seq),len(bow)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='10' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [10/10 00:02<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pickle\n",
    "# save_file='2020_10_08_receiver'\n",
    "# save_skip=2500\n",
    "num_episodes = 5000\n",
    "mb = progress_bar(range(num_episodes))\n",
    "p_skip = 250\n",
    "\n",
    "# env = maze_game([maze_list[0]])\n",
    "env = maze_game(maze_list)\n",
    "\n",
    "w_hist = []\n",
    "r_hist = []\n",
    "\n",
    "for i_episode in mb:\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "#     env.set_target(np.array([4,4]))\n",
    "\n",
    "    c_actions = state_to_actions(env.state, limit_n=4)\n",
    "    sentence = actions_to_words(c_actions)\n",
    "    \n",
    "    sentence += ['']*(sequence_length - len(sentence))\n",
    "    \n",
    "    last_state = encode_bow(sentence)\n",
    "    state = last_state\n",
    "    \n",
    "    loss = []\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        valid_actions = env.valid_actions()\n",
    "        \n",
    "        action = select_action(state, valid_actions=valid_actions)\n",
    "    \n",
    "        actions = [Actions(i.item()) for i in action]\n",
    "    \n",
    "        a_reward = 0\n",
    "        for a,b in enumerate(actions):\n",
    "            if a<len(c_actions) and b == c_actions[a]:\n",
    "                a_reward += 0.01\n",
    "            elif a>=len(c_actions) and b != Actions.NONE:\n",
    "                a_reward -= 0.01\n",
    "            else:\n",
    "                a_reward -= -0.01\n",
    "                \n",
    "        r = max_actions\n",
    "        for a,b in enumerate(actions):\n",
    "            if b != Actions.NONE:\n",
    "                r = a\n",
    "                \n",
    "        actions = actions[:r+1]\n",
    "        \n",
    "        action = action.view(max_actions,1)\n",
    "        \n",
    "        _, reward, done, win = env.step(actions)\n",
    "        reward += a_reward\n",
    "        \n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        r_hist += [reward.item()]\n",
    "\n",
    "\n",
    "        if not done:\n",
    "            c_actions = state_to_actions(env.state, limit_n=4)\n",
    "            sentence = actions_to_words(c_actions)\n",
    "            sentence += ['']*(sequence_length - len(sentence))\n",
    "            next_state = encode_bow(sentence)\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        loss += [optimize_model()]\n",
    "\n",
    "        if done:\n",
    "            w_hist.append(win)\n",
    "            break\n",
    "    \n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    \n",
    "#     if (i_episode+1)%save_skip==0:\n",
    "#         with open(f'models/{save_file}_policy.pkl', 'wb') as f:\n",
    "#             pickle.dump(policy_net, f)\n",
    "#         with open(f'models/{save_file}_target.pkl', 'wb') as f:\n",
    "#             pickle.dump(target_net, f)\n",
    "        \n",
    "        \n",
    "    if (i_episode+1)%p_skip == 0:\n",
    "        print(f\"Episode: {i_episode:4d}/{num_episodes-1} | Rounds: {env.rounds:3d} | Steps: {env.steps:3d} | Wins: {sum(w_hist):4d} | Epsilon: {eps_threshold:.3f} | Avg Episode Reward: {f'{np.mean(r_hist[-(t+1):]):.4f}':>7s} | Avg Reward: {f'{np.mean(r_hist[-200:]):.4f}':>7s} | Total Winrate: {f'{sum(w_hist)/(i_episode+1)*100:>3.1f}':>5s}% | Recent Winrate: {f'{sum(w_hist[-p_skip:])/(p_skip)*100:>3.1f}':>5s}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'go two spaces east'\n",
      "[<Actions.NONE: 4>, <Actions.NONE: 4>, <Actions.NONE: 4>, <Actions.NONE: 4>]\n",
      "Episode 9: '['go', 'two', 'spaces', 'east']', [<Actions.NONE: 4>, <Actions.NONE: 4>, <Actions.NONE: 4>, <Actions.NONE: 4>]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = maze_game([maze_list[0]])\n",
    "env.reset()\n",
    "env.set_target(np.array([4,4]))\n",
    "env.set_position(np.array([2,3]))\n",
    "\n",
    "sentence = actions_to_words(state_to_actions(env.state))\n",
    "sentence = 'go two spaces east'.split(' ')\n",
    "enc = encode_bow(sentence)\n",
    "\n",
    "action = policy_net(enc[None]).max(2)[1].view(1, max_actions, 1, 1)[0]\n",
    "actions = [Actions(i.item()) for i in action]\n",
    "\n",
    "print(f\"Sentence: '{' '.join(sentence)}'\")\n",
    "if len(actions) == 0:\n",
    "    actions = [Actions.NONE]\n",
    "env.step(actions)\n",
    "env.show()\n",
    "print(actions)\n",
    "\n",
    "print(f\"Episode {i_episode}: '{sentence}', {actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='10000' class='' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [10000/10000 02:41<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelof\\.conda\\envs\\py38\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:734: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ..\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:1264.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Win Rate: 100.00%, Avg Steps: 3.8727\n",
      "Maze 0: 100.00%, 5.02 steps\n",
      "Maze 1: 100.00%, 4.36 steps\n",
      "Maze 2: 100.00%, 3.22 steps\n",
      "Maze 3: 100.00%, 4.70 steps\n",
      "Maze 4: 100.00%, 3.18 steps\n",
      "Maze 5: 100.00%, 2.66 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-66ad4c6d6bab>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  print(f\"Total Win Rate: {np.mean(np.concatenate(np.array(w_hist)))*100:.2f}%, Avg Steps: {np.mean(np.concatenate(np.array(s_hist)))}\")\n"
     ]
    }
   ],
   "source": [
    "mb = progress_bar(range(10000))\n",
    "# m = [maze_list[5]]\n",
    "m = maze_list\n",
    "w_hist = [[] for i in range(len(m))]\n",
    "s_hist = [[] for i in range(len(m))]\n",
    "for i in mb:\n",
    "    maze_num = np.random.randint(0, len(m))\n",
    "    win,steps = play_game(policy_net, m[maze_num], epsilon=0.01, target=np.array([4,4]))\n",
    "    w_hist[maze_num] = np.append(w_hist[maze_num], win)\n",
    "    s_hist[maze_num] = np.append(s_hist[maze_num], steps)\n",
    "\n",
    "print(f\"Total Win Rate: {np.mean(np.concatenate(np.array(w_hist)))*100:.2f}%, Avg Steps: {np.mean(np.concatenate(np.array(s_hist)))}\")\n",
    "for i,j in enumerate(w_hist):\n",
    "    print(f\"Maze {i}: {f'{np.mean(j)*100:.2f}':>6s}%, {np.mean(s_hist[i]):.2f} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('models/sender_policy.pkl', 'wb') as f:\n",
    "    pickle.dump(policy_net, f)\n",
    "with open('models/sender_target.pkl', 'wb') as f:\n",
    "    pickle.dump(target_net, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('models/sender.pkl', 'rb') as f:\n",
    "    policy_net = pickle.load(f)\n",
    "with open('models/sender.pkl', 'rb') as f:\n",
    "    target_net = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
